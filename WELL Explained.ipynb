{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from IPython.display import Image\n",
    "import seaborn as sns\n",
    "sns.set(style=\"white\", color_codes=True)\n",
    "\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data=pd.read_csv('Downloads/iris.csv')\n",
    "# #data.head(1)\n",
    "# data.drop('Id',axis=1,inplace=True)\n",
    "# data['Species']=np.where(data['Species']=='Iris-setosa',0,np.where(data['Species']=='Iris-versicolor',1,2))\n",
    "# #data.head(1)\n",
    "# #print(data.info())\n",
    "# X=data.values[:,:4]\n",
    "# #print X\n",
    "# y=data.values[:,4]\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "# print X_test\n",
    "# #data.shape\n",
    "# # data.head(1)\n",
    "# #data['Species']=np.where(data['Species']=='Iris-setosa',0,np.where(data['Species']=='Iris-versicolor',1,2))\n",
    "# # for i in range(0,2999):\n",
    "# #     if (data['Species']=='Iris-setosa'):\n",
    "# #         data['Species']=0\n",
    "        \n",
    "# #data['Species'] = data['Species'].map(lambda Iris-setosa:0)#,1,2])\n",
    "# #y=data.values[:,4]\n",
    "# #print(type(y))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-ff0030da3bc7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mstdsc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mX_train_std\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstdsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#fit nd then transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mX_test_std\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstdsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#only transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#print(X_test_std)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "##ON Data-Standardization(-ve and +ve balanced values-Uniform distribution-zero mean-less sensitive to outliers)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "stdsc = StandardScaler()\n",
    "X_train_std = stdsc.fit_transform(X_train)#fit nd then transform\n",
    "X_test_std = stdsc.transform(X_test)#only transform\n",
    "#print(X_test_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.41666667  0.33333333  0.68965517  0.95833333]\n",
      " [ 0.47222222  0.08333333  0.5         0.375     ]\n",
      " [ 0.33333333  0.91666667  0.05172414  0.04166667]\n",
      " [ 0.83333333  0.375       0.89655172  0.70833333]\n",
      " [ 0.19444444  0.58333333  0.06896552  0.04166667]\n",
      " [ 0.55555556  0.54166667  0.84482759  1.        ]\n",
      " [ 0.19444444  0.625       0.03448276  0.08333333]\n",
      " [ 0.66666667  0.45833333  0.62068966  0.58333333]\n",
      " [ 0.69444444  0.33333333  0.63793103  0.54166667]\n",
      " [ 0.5         0.33333333  0.5         0.5       ]\n",
      " [ 0.5         0.25        0.77586207  0.54166667]\n",
      " [ 0.58333333  0.5         0.5862069   0.58333333]\n",
      " [ 0.5         0.33333333  0.62068966  0.45833333]\n",
      " [ 0.61111111  0.33333333  0.60344828  0.58333333]\n",
      " [ 0.5         0.375       0.62068966  0.54166667]\n",
      " [ 0.16666667  0.45833333  0.06896552  0.        ]\n",
      " [ 0.47222222  0.375       0.5862069   0.58333333]\n",
      " [ 0.33333333  0.25        0.56896552  0.45833333]\n",
      " [ 0.13888889  0.41666667  0.05172414  0.08333333]\n",
      " [ 0.30555556  0.79166667  0.03448276  0.125     ]\n",
      " [ 0.36111111  0.33333333  0.65517241  0.79166667]\n",
      " [ 0.36111111  0.41666667  0.5862069   0.58333333]\n",
      " [ 0.13888889  0.58333333  0.13793103  0.04166667]\n",
      " [ 0.02777778  0.375       0.05172414  0.04166667]\n",
      " [ 0.52777778  0.33333333  0.63793103  0.70833333]\n",
      " [ 0.08333333  0.66666667 -0.01724138  0.04166667]\n",
      " [ 0.22222222  0.75        0.13793103  0.125     ]\n",
      " [ 0.52777778  0.375       0.55172414  0.5       ]\n",
      " [ 0.19444444  0.125       0.37931034  0.375     ]\n",
      " [ 0.19444444  0.58333333  0.0862069   0.125     ]\n",
      " [ 0.58333333  0.45833333  0.75862069  0.70833333]\n",
      " [ 0.30555556  0.41666667  0.5862069   0.58333333]\n",
      " [ 0.25        0.625       0.06896552  0.04166667]\n",
      " [ 0.5         0.41666667  0.65517241  0.70833333]\n",
      " [ 0.58333333  0.33333333  0.77586207  0.875     ]\n",
      " [ 0.25        0.29166667  0.48275862  0.54166667]\n",
      " [ 0.38888889  0.75        0.10344828  0.08333333]\n",
      " [ 0.47222222  0.29166667  0.68965517  0.625     ]\n",
      " [ 0.44444444  0.41666667  0.53448276  0.58333333]\n",
      " [ 0.41666667  0.25        0.5         0.45833333]\n",
      " [ 0.69444444  0.41666667  0.75862069  0.83333333]\n",
      " [ 0.11111111  0.5         0.03448276  0.04166667]\n",
      " [ 0.72222222  0.45833333  0.68965517  0.91666667]\n",
      " [ 0.19444444  0.625       0.0862069   0.20833333]\n",
      " [ 0.30555556  0.70833333  0.06896552  0.04166667]]\n"
     ]
    }
   ],
   "source": [
    "##ON Data-Normalization[0-1]\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "mms = MinMaxScaler()\n",
    "X_train_norm = mms.fit_transform(X_train)\n",
    "X_test_norm = mms.transform(X_test)\n",
    "print(X_test_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training accuracy:', 0.99047619047619051)\n",
      "('Test accuracy:', 0.97777777777777775)\n",
      "('Training accuracy:', 0.94285714285714284)\n",
      "('Test accuracy:', 0.9555555555555556)\n",
      "('Training accuracy:', 0.99047619047619051)\n",
      "('Test accuracy:', 0.93333333333333335)\n",
      "('Training accuracy:', 0.94285714285714284)\n",
      "('Test accuracy:', 0.88888888888888884)\n"
     ]
    }
   ],
   "source": [
    "##ON Model-Regularization-L1 TO AVOID OVERFITTING when high dimensional data-overfitting-L1 gives Sparse solution i.e.remove irrelevant features\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# WITHOUT REGULARIZATION \n",
    "lr1=LogisticRegression(C=1000)\n",
    "lr1.fit(X_train_std, y_train)\n",
    "print('Training accuracy:', lr1.score(X_train_std, y_train))\n",
    "print('Test accuracy:', lr1.score(X_test_std, y_test))\n",
    "\n",
    "##AFTER REGULARIZATION\n",
    "lr=LogisticRegression(penalty='l2',C=10)\n",
    "lr.fit(X_train_std, y_train)\n",
    "print('Training accuracy:', lr.score(X_train_std, y_train))\n",
    "print('Test accuracy:', lr.score(X_test_std, y_test))\n",
    "\n",
    "##REGULARIZATION ON NON STANDARDIZED DATA\n",
    "lr2=LogisticRegression(penalty='l2',C=10)\n",
    "lr2.fit(X_train, y_train)\n",
    "print('Training accuracy:', lr2.score(X_train, y_train))\n",
    "print('Test accuracy:', lr2.score(X_test, y_test))\n",
    "\n",
    "##SIMPLE LOGISTIC REGRESSION(BEST RESULTS)\n",
    "lr3=LogisticRegression()\n",
    "lr3.fit(X_train, y_train)\n",
    "print('Training accuracy:', lr3.score(X_train, y_train))\n",
    "print('Test accuracy:', lr3.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training accuracy:', 0.97142857142857142)\n",
      "('Test accuracy:', 0.97777777777777775)\n"
     ]
    }
   ],
   "source": [
    "##SVM IMPLEMENTATION #Linear kernel better than rbf here\n",
    "##LOOK BOTH GAMMA AND C\n",
    "from sklearn.svm import SVC\n",
    "svm = SVC(kernel='linear', random_state=0, gamma=0.10, C=10)\n",
    "svm.fit(X_train_norm, y_train)\n",
    "print('Training accuracy:', svm.score(X_train_norm, y_train))\n",
    "print('Test accuracy:', svm.score(X_test_norm, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training accuracy:', 0.98095238095238091)\n",
      "('Test accuracy:', 0.97777777777777775)\n"
     ]
    }
   ],
   "source": [
    "##Decision Tree \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "tree = DecisionTreeClassifier(criterion='entropy',max_depth=3, random_state=0)\n",
    "tree.fit(X_train_std, y_train)\n",
    "print('Training accuracy:', tree.score(X_train_std, y_train))\n",
    "print('Test accuracy:', tree.score(X_test_std, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training accuracy:', 1.0)\n",
      "('Test accuracy:', 0.97777777777777775)\n"
     ]
    }
   ],
   "source": [
    "## RANDOM FOREST\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest = RandomForestClassifier(criterion='entropy',\n",
    "n_estimators=10,\n",
    "random_state=1,\n",
    "n_jobs=2)\n",
    "forest.fit(X_train, y_train)\n",
    "print('Training accuracy:', forest.score(X_train, y_train))\n",
    "print('Test accuracy:', forest.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training accuracy:', 1.0)\n",
      "('Test accuracy:', 0.97777777777777775)\n"
     ]
    }
   ],
   "source": [
    "##KMEANS CLASSIFIER## NORMALISATION AND STANDARDIZATION BOTH DEGRADE RESULTS\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5, p=2,\n",
    "metric='minkowski')\n",
    "knn.fit(X_train, y_train)\n",
    "print('Training accuracy:', forest.score(X_train, y_train))\n",
    "print('Test accuracy:', forest.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91111111111111109"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##FEATURE EXTRACTION-PCA AND PIPELINE(SERIES OF STEPS TAKE PLACE TOGETHER & THEN RESULT ACHIEVED) CONCEPT\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "pipe_lr = Pipeline([('scl', StandardScaler()),('pca', PCA(n_components=3)),\n",
    "('clf', LogisticRegression(penalty='l1',random_state=1))])\n",
    "pipe_lr.fit(X_train, y_train)\n",
    "pipe_lr.score(X_test, y_test)\n",
    "##Max accuracy achieved of all the cases on using penalty parameter as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.10737218  0.025838    0.45810717  0.40868265]\n"
     ]
    }
   ],
   "source": [
    "##FEATURE SELECTION USING RANDOM FOREST\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest = RandomForestClassifier(n_estimators=10000,random_state=0,n_jobs=-1)\n",
    "forest.fit(X_train, y_train)\n",
    "importances = forest.feature_importances_\n",
    "# forest.fit(X_train_std, y_train)\n",
    "# importances2 = forest.feature_importances_\n",
    "##BOTH IMPORTANCES RESULT SAME\n",
    "print(importances)#,importances2)*111*1*5*4#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #relations\n",
    "# #data.info()\n",
    "#EEspecies_seLcat1 = data[data['Species']==0]['PetalLengthCm'].value_counts()\n",
    "#EE species_seLcat2 = data[data['Species']==1]['SepalWidthCm'].value_counts()\n",
    "# EE species_seLcat3 = data[data['Species']==2]['SepalWidthCm'].value_counts()\n",
    "#print(species_seLcat1)\n",
    "# df = pd.DataFrame([species_seLcat3])#,species_seLcat2,species_seLcat3])\n",
    "# # df = pd.DataFrame([species_seLcat1,species_seLcat2,species_seLcat3])\n",
    "# # df.index = ['1st category','2nd category','3rd category']\n",
    "# df.plot(kind='bar', figsize=(15,8))\n",
    "#EEdata.plot(kind=\"scatter\", x=\"PetalLengthCm\", y=\"PetalWidthCm\")\n",
    "#EEg = sns.factorplot(\"PetalLengthCm\",\"PetalWidthCm\", \"Species\",data=data,kind='swarm',size=10)\n",
    "#equivalent to g but factorplot does not require to specify sequence\n",
    "# sns.FacetGrid(data, hue=\"Species\", size=5) \\\n",
    "#    .map(plt.scatter, \"SepalLengthCm\", \"SepalWidthCm\") \\\n",
    "#    .add_legend()\n",
    "#EEsns.boxplot(x=\"Species\", y=\"SepalLengthCm\", data=data)\n",
    "#EESaving the resulting axes as ax each time causes the resulting plot to be shown\n",
    "# on top of the previous axes\n",
    "#EEsns.boxplot(x=\"Species\", y=\"PetalLengthCm\", data=data)\n",
    "#EEax=sns.stripplot(x=\"Species\", y=\"PetalLengthCm\", data=data, jitter=True, edgecolor=\"gray\",size=10)\n",
    "#EEsns.violinplot(x=\"Species\", y=\"PetalLengthCm\", data=data, size=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#EEsns.violinplot(x=\"Species\", y=\"PetalLengthCm\", data=data, size=6)\n",
    "#EE y axis in below gives no of times a particular val occcurs\n",
    "#EEsns.FacetGrid(data, hue=\"Species\", size=6) \\\n",
    "#   .map(sns.kdeplot, \"PetalLengthCm\") \\\n",
    "#   .add_legend()\n",
    "#EEsns.pairplot( hue=\"Species\",data=data)\n",
    "#EEesns.pairplot(data=data, hue=\"Species\", size=3, diag_kind='kde')\n",
    "# knn=KNeighborsClassifier(n_neighbors=3)\n",
    "# scores = cross_val_score(knn,X,y,cv=10,scoring='accuracy')\n",
    "# # scores=cross_val_score(knn,X,y,cv=10,scoring='accuracy')\n",
    "# print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          1.22474487  1.22474487  1.22474487  1.22474487  0.\n",
      "  1.22474487  0.          0.          1.22474487  1.22474487  1.22474487\n",
      "  1.22474487  0.          1.22474487  0.         -1.22474487  1.22474487\n",
      "  0.          0.          0.          0.          1.22474487 -1.22474487\n",
      " -1.22474487  1.22474487  0.         -1.22474487 -1.22474487  0.\n",
      " -1.22474487  1.22474487  0.         -1.22474487  0.          1.22474487\n",
      "  0.         -1.22474487  1.22474487  1.22474487  1.22474487  1.22474487\n",
      " -1.22474487 -1.22474487  1.22474487  1.22474487 -1.22474487  1.22474487\n",
      " -1.22474487  1.22474487  1.22474487 -1.22474487 -1.22474487  1.22474487\n",
      " -1.22474487 -1.22474487 -1.22474487  0.          1.22474487  1.22474487\n",
      " -1.22474487 -1.22474487 -1.22474487  0.          0.         -1.22474487\n",
      " -1.22474487  0.         -1.22474487  1.22474487  0.          1.22474487\n",
      "  0.         -1.22474487  1.22474487 -1.22474487  1.22474487 -1.22474487\n",
      " -1.22474487  1.22474487 -1.22474487  1.22474487  0.          0.          0.\n",
      "  1.22474487  1.22474487  0.          0.         -1.22474487  0.\n",
      "  1.22474487  1.22474487 -1.22474487  0.          0.          0.          0.\n",
      " -1.22474487 -1.22474487 -1.22474487  1.22474487  0.          1.22474487\n",
      " -1.22474487]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/data.py:586: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/data.py:649: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=0)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "sc.fit(y)\n",
    "#EEX_train_std = sc.transform(X_train)\n",
    "#EEX_test_std =sc.transform(X_test)\n",
    "yyy=sc.transform(y_train)\n",
    "print yyy\n",
    "\n",
    "\n",
    "## NOTICE this way  print('There are {} samples in the training set and {} samples in the test set'.format(\n",
    "##X_train.shape[0], X_test.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
